{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESGF Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import dask\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore', xr.SerializationWarning)\n",
    "xr.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Cluster\n",
    "\n",
    "Optional but makes things go faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=8)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Searching ESGF API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import requests\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# API AT: https://github.com/ESGF/esgf.github.io/wiki/ESGF_Search_REST_API#results-pagination\n",
    "\n",
    "def check_doc_for_malformed_id(d):\n",
    "    source_id = d['source_id'][0]\n",
    "    expt_id = d['experiment_id'][0]\n",
    "    if not  f\"{source_id}_{expt_id}\" in d['id']:\n",
    "        raise ValueError(f\"Dataset id {d['id']} is malformed\")\n",
    "                         \n",
    "def maybe_squeze_values(d):\n",
    "    def _maybe_squeeze(value):\n",
    "        if isinstance(value, str):\n",
    "            return value\n",
    "        try:\n",
    "            if len(value)==1:\n",
    "                return value[0]\n",
    "        except TypeError:\n",
    "            return(value)\n",
    "    return {k: _maybe_squeeze(v) for k, v in d.items()}\n",
    "                         \n",
    "def get_request(server, verbose=False, **payload):\n",
    "    client = requests.session()\n",
    "    url_keys = [] \n",
    "    url_keys = [\"{}={}\".format(k, payload[k]) for k in payload]\n",
    "    url = \"{}/?{}\".format(server, \"&\".join(url_keys))\n",
    "    if verbose:\n",
    "        print(url)\n",
    "    r = client.get(url)\n",
    "    r.raise_for_status()\n",
    "    resp = r.json()[\"response\"]\n",
    "    return resp\n",
    "\n",
    "def get_page_dataframe(server, expected_size, files_type='OPENDAP', offset=0,\n",
    "                       filter_server_url=None, verbose=False,\n",
    "                       **payload):\n",
    "                         \n",
    "    resp = get_request(server, offset=offset, verbose=verbose, **payload)\n",
    "\n",
    "    docs = resp[\"docs\"]\n",
    "    assert len(docs) == expected_size\n",
    "\n",
    "    all_files = []\n",
    "    for d in docs:\n",
    "        try:\n",
    "            check_doc_for_malformed_id(d)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        dataset_id = d['dataset_id']\n",
    "        item = OrderedDict(dataset_id=dataset_id, id=d['id'])\n",
    "        target_urls = d.pop('url')\n",
    "        item.update(maybe_squeze_values(d))\n",
    "        for f in target_urls:\n",
    "            sp = f.split(\"|\")\n",
    "            if sp[-1] == files_type:\n",
    "                opendap_url = sp[0].replace('.html', '')\n",
    "                if filter_server_url is None or filter_server_url in opendap_url:\n",
    "                    item.update({f'{files_type}_url': opendap_url})\n",
    "                    all_files.append(item)\n",
    "                         \n",
    "    # dropping duplicates on checksum removes all identical files\n",
    "    return pd.DataFrame(all_files).drop_duplicates(subset='checksum')\n",
    "                         \n",
    "get_page_dataframe_d = dask.delayed(get_page_dataframe)\n",
    "                         \n",
    "def esgf_search(server=\"https://esgf-node.llnl.gov/esg-search/search\",\n",
    "                files_type=\"OPENDAP\", local_node=True, project=\"CMIP6\",\n",
    "                # this option should not be necessary with local_node=True\n",
    "                filter_server_url=None,\n",
    "                verbose=False, format=\"application%2Fsolr%2Bjson\",\n",
    "                use_csrf=False, **search):\n",
    "                         \n",
    "    payload = search\n",
    "    payload[\"project\"] = project\n",
    "    payload[\"type\"]= \"File\"\n",
    "    if local_node:\n",
    "        payload[\"distrib\"] = \"false\"\n",
    "    if use_csrf:\n",
    "        client.get(server)\n",
    "        if 'csrftoken' in client.cookies:\n",
    "            # Django 1.6 and up\n",
    "            csrftoken = client.cookies['csrftoken']\n",
    "        else:\n",
    "            # older versions\n",
    "            csrftoken = client.cookies['csrf']\n",
    "        payload[\"csrfmiddlewaretoken\"] = csrftoken\n",
    "\n",
    "    payload[\"format\"] = format\n",
    "\n",
    "    init_resp = get_request(server, offset=0, verbose=verbose, **payload)\n",
    "    num_found = int(init_resp[\"numFound\"])\n",
    "    page_size = len(init_resp[\"docs\"])\n",
    "                         \n",
    "    offset = 0\n",
    "    files_type = files_type.upper()\n",
    "                         \n",
    "    \n",
    "    all_frames = []         \n",
    "    for offset in range(0, num_found, page_size):\n",
    "        \n",
    "        expected_size = page_size if offset <= (num_found - page_size) else (num_found - offset)         \n",
    "        df_d = get_page_dataframe_d(server, expected_size, files_type=files_type, offset=offset,\n",
    "                                    filter_server_url=filter_server_url, verbose=verbose,\n",
    "                                    **payload)\n",
    "\n",
    "        all_frames.append(df_d)                        \n",
    "    return pd.concat(*dask.compute(all_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we be searching more models in order to get more data?\n",
    "\n",
    "## 233 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', variable=\"ta\", table_id='Amon', filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "## 1179 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', variable=\"ta\")\n",
    "\n",
    "## 2615 files\n",
    "files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', institution_id='NASA-GISS',\n",
    "                    frequency='mon',\n",
    "                    table_id='Amon', latest='true', filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "## 13453 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP',\n",
    "#                    table_id='Amon', latest='true', filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "## 83273 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', latest='true', frequency='mon',\n",
    "#                    filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally save results for re-loading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4233 entries, 0 to 2\n",
      "Data columns (total 53 columns):\n",
      "dataset_id                    4233 non-null object\n",
      "id                            4233 non-null object\n",
      "version                       4233 non-null object\n",
      "activity_drs                  4233 non-null object\n",
      "activity_id                   4233 non-null object\n",
      "branch_method                 4233 non-null object\n",
      "cf_standard_name              4233 non-null object\n",
      "checksum                      4233 non-null object\n",
      "checksum_type                 4233 non-null object\n",
      "citation_url                  4233 non-null object\n",
      "data_node                     4233 non-null object\n",
      "data_specs_version            4233 non-null object\n",
      "dataset_id_template_          4233 non-null object\n",
      "directory_format_template_    4233 non-null object\n",
      "experiment_id                 4233 non-null object\n",
      "experiment_title              4233 non-null object\n",
      "frequency                     4233 non-null object\n",
      "further_info_url              4233 non-null object\n",
      "grid                          4233 non-null object\n",
      "grid_label                    4233 non-null object\n",
      "index_node                    4233 non-null object\n",
      "instance_id                   4233 non-null object\n",
      "institution_id                4233 non-null object\n",
      "latest                        4233 non-null bool\n",
      "master_id                     4233 non-null object\n",
      "member_id                     4233 non-null object\n",
      "mip_era                       4233 non-null object\n",
      "model_cohort                  4233 non-null object\n",
      "nominal_resolution            4233 non-null object\n",
      "pid                           4233 non-null object\n",
      "product                       4233 non-null object\n",
      "project                       4233 non-null object\n",
      "realm                         4233 non-null object\n",
      "replica                       4233 non-null bool\n",
      "size                          4233 non-null int64\n",
      "source_id                     4233 non-null object\n",
      "source_type                   4233 non-null object\n",
      "sub_experiment_id             4233 non-null object\n",
      "table_id                      4233 non-null object\n",
      "timestamp                     4233 non-null object\n",
      "title                         4233 non-null object\n",
      "tracking_id                   4233 non-null object\n",
      "type                          4233 non-null object\n",
      "variable                      4233 non-null object\n",
      "variable_id                   4233 non-null object\n",
      "variable_long_name            4233 non-null object\n",
      "variable_units                4233 non-null object\n",
      "variant_label                 4233 non-null object\n",
      "_version_                     4233 non-null int64\n",
      "retracted                     4233 non-null bool\n",
      "_timestamp                    4233 non-null object\n",
      "score                         4233 non-null float64\n",
      "OPENDAP_url                   4233 non-null object\n",
      "dtypes: bool(3), float64(1), int64(2), object(47)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "files.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files.to_csv('cmip6_Amon_monthly.csv', index=False)\n",
    "#pd.read_csv('cmip6_Amon_monthly.csv.gz').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.9 ms, sys: 7.01 ms, total: 38.9 ms\n",
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "dataset_fields = ['institution_id', 'source_id', 'experiment_id', 'table_id', 'grid_label']\n",
    "all_dsets = {}\n",
    "for dset_keys, dset_files in files.groupby(dataset_fields):\n",
    "    dset_id = '.'.join(dset_keys)\n",
    "    print(dset_id)\n",
    "    for var_id, var_files in dset_files.groupby('variable_id'):\n",
    "        n_members = var_files.member_id.nunique()\n",
    "        print(f' * {var_id}: {n_members} members')\n",
    "        for m_id, m_files in var_files.groupby('member_id'):\n",
    "            print(f'    - {m_id}: {len(m_files)} files')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazily Loading of Datasets\n",
    "\n",
    "Functions to build xarray datasets out of individual opendap endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "def dict_union(*dicts, merge_keys=['history', 'further_info_url'],\n",
    "               drop_keys=['DODS_EXTRA.Unlimited_Dimension']):\n",
    "    if len(dicts) > 2:\n",
    "        return reduce(dict_union, dicts)\n",
    "    elif len(dicts)==2:\n",
    "        d1, d2 = dicts\n",
    "        d = type(d1)()\n",
    "        # union\n",
    "        all_keys = set(d1) | set(d2)\n",
    "        for k in all_keys:\n",
    "            v1 = d1.get(k)\n",
    "            v2 = d2.get(k)\n",
    "            if (v1 is None and v2 is None) or k in drop_keys:\n",
    "                pass\n",
    "            elif v1 is None:\n",
    "                d[k] = v2\n",
    "            elif v2 is None:\n",
    "                d[k] = v1\n",
    "            elif v1==v2:\n",
    "                d[k] = v1\n",
    "            elif k in merge_keys:\n",
    "                d[k] = '\\n'.join([v1, v2])\n",
    "        return d\n",
    "    elif len(dicts)==1:\n",
    "        return dicts[0]\n",
    "\n",
    "def set_bnds_as_coords(ds):\n",
    "    new_coords_vars = [var for var in ds.data_vars if 'bnds' in var or 'bounds' in var]\n",
    "    ds = ds.set_coords(new_coords_vars)\n",
    "    return ds\n",
    "\n",
    "def fix_climatology_time(ds):\n",
    "    for dim in ds.dims:\n",
    "        if 'climatology' in ds[dim].attrs:\n",
    "            ds = ds.rename({dim: dim + '_climatology'})\n",
    "    return ds\n",
    "\n",
    "def set_coords(ds):\n",
    "    # there should only be one variable per file\n",
    "    # everything else is coords\n",
    "    varname = ds.attrs['variable_id']\n",
    "    coord_vars = set(ds.data_vars) - {varname}\n",
    "    ds = ds.set_coords(coord_vars)\n",
    "    ds = fix_climatology_time(ds)\n",
    "    return(ds)\n",
    "\n",
    "def open_dataset(url):\n",
    "    ds = xr.open_dataset(url, chunks={'time': 'auto'}, decode_times=False)\n",
    "    ds.attrs['history'] = f\"{datetime.now()} xarray.open_dataset('{url}')\"\n",
    "    ds = set_coords(ds)\n",
    "    return ds\n",
    "\n",
    "open_dataset_delayed = dask.delayed(open_dataset)\n",
    "\n",
    "def concat_timesteps(dsets, timevar='time'):\n",
    "    if len(dsets)==1:\n",
    "        return dsets[0]\n",
    "    \n",
    "    attrs = dict_union(*[ds.attrs for ds in dsets])\n",
    "    \n",
    "    # for nd-coordinates without time from first ensemble member to simplify merge\n",
    "    first = dsets[0]\n",
    " \n",
    "    def drop_unnecessary_coords(ds):\n",
    "        ndcoords = set(ds.coords) - set(ds.dims)\n",
    "        ndcoords_drop = [coord for coord in ndcoords if timevar not in ds[coord].dims]\n",
    "        return ds.drop(ndcoords_drop)\n",
    "    \n",
    "    rest = [drop_unnecessary_coords(ds) for ds in dsets[1:]]\n",
    "    objs_to_concat = [first] + rest\n",
    "    \n",
    "    ds = xr.concat(objs_to_concat, dim=timevar, coords='minimal')\n",
    "    attrs['history'] += f\"\\n{datetime.now()} xarray.concat(<ALL_TIMESTEPS>, dim='{timevar}', coords='minimal')\"\n",
    "    ds.attrs = attrs\n",
    "    return ds\n",
    "    \n",
    "def concat_ensembles(member_dsets, member_ids, join='outer'):\n",
    "    if len(member_dsets)==1:\n",
    "        return member_dsets[0]\n",
    "    concat_dim = xr.DataArray(member_ids, dims='member_id', name='member_id')\n",
    "    \n",
    "    # warning: this function broke for the IPSL historical o3 variable because it\n",
    "    # contained a mix of frequencies (monthly and climatology)\n",
    "    # this was fixed by adding frequency=\"mon\" to the search\n",
    "    \n",
    "    # merge attributes\n",
    "    attrs = dict_union(*[ds.attrs for ds in member_dsets])\n",
    "    \n",
    "    # align first to deal with the fact that some ensemble members have different lengths\n",
    "    # inner join keeps only overlapping segments of each ensemble\n",
    "    # outer join gives us the longest possible record\n",
    "    member_dsets_aligned = xr.align(*member_dsets, join=join)\n",
    "    \n",
    "    # keep only coordinates from first ensemble member to simplify merge\n",
    "    first = member_dsets_aligned[0]\n",
    "    rest = [mds.reset_coords(drop=True) for mds in member_dsets_aligned[1:]]\n",
    "    objs_to_concat = [first] + rest\n",
    "    \n",
    "    ds = xr.concat(objs_to_concat, dim=concat_dim, coords='minimal')\n",
    "    attrs['history'] += f\"\\n{datetime.now()} xarray.concat(<ALL_MEMBERS>, dim='member_id', coords='minimal')\"\n",
    "    ds.attrs = attrs\n",
    "    return ds\n",
    "\n",
    "def merge_vars(ds1, ds2):\n",
    "    # merge two datasets at a time - designed for recursive merging\n",
    "    # drop all variables from second that already exist in first's coordinates\n",
    "\n",
    "    # I can't believe xarray doesn't have a merge that keeps attrs\n",
    "    attrs = dict_union(ds1.attrs, ds2.attrs)\n",
    "    \n",
    "    # non dimension coords\n",
    "    # could be skipping over \n",
    "    ds1_ndcoords = set(ds1.coords) - set(ds1.dims)\n",
    "    \n",
    "    # edge case for variable 'ps', which is a coordinate in some datasets\n",
    "    # and a data_var in its own dataset\n",
    "    ds2_dropvars = set(ds2.variables).intersection(ds1_ndcoords)\n",
    "    ds2_drop = ds2.drop(ds2_dropvars)\n",
    "    \n",
    "    ds = xr.merge([ds1, ds2_drop])\n",
    "    ds.attrs = attrs\n",
    "    return ds\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "def merge_recursive(dsets):\n",
    "    dsm = reduce(merge_vars, dsets)\n",
    "    dsm.attrs['history'] += f\"\\n{datetime.now()} xarray.merge(<ALL_VARIABLES>)\"\n",
    "    \n",
    "    # fix further_info_url\n",
    "    fi_urls = set(dsm.attrs['further_info_url'].split('\\n'))\n",
    "    dsm.attrs['further_info_url'] = '\\n'.join(fi_urls)\n",
    "    \n",
    "    # rechunk\n",
    "    chunks = {'time': 'auto'}\n",
    "    if 'member_id' in dsm.dims:\n",
    "        chunks.update({'member_id': 1})\n",
    "    if 'time_climatology' in dsm.dims:\n",
    "        chunks.update({'time_climatology': 1})\n",
    "    return dsm.chunk(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build datasets from search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fields = ['institution_id', 'source_id', 'experiment_id', 'table_id', 'grid_label']\n",
    "all_dsets = {}\n",
    "n=0\n",
    "for dset_keys, dset_files in tqdm(files.groupby(dataset_fields), desc='dataset'):\n",
    "    dset_id = '.'.join(dset_keys)\n",
    "    n+=1\n",
    "    #if n==1:\n",
    "    #    continue\n",
    "    \n",
    "    all_member_dsets = []\n",
    "    all_member_ids = []\n",
    "    \n",
    "    # first build a nested list of delayed datasets\n",
    "    for var_id, var_files in dset_files.groupby('variable_id'):\n",
    "        member_dsets = []\n",
    "        member_ids = []\n",
    "        for m_id, m_files in var_files.groupby('member_id'):\n",
    "            member_ids.append(m_id)\n",
    "            member_dsets.append([open_dataset_delayed(url) for url in m_files.OPENDAP_url])\n",
    "        all_member_dsets.append(member_dsets)\n",
    "        all_member_ids.append(member_ids)\n",
    "    \n",
    "    # now compute them all in parallel\n",
    "    all_member_dsets_c = dask.compute(*all_member_dsets, retries=5)\n",
    "    \n",
    "    # and merge them\n",
    "    var_dsets = [concat_ensembles([concat_timesteps(time_dsets) for time_dsets in member_dsets],\n",
    "                                  member_ids)\n",
    "             for member_dsets, member_ids in zip(\n",
    "                 tqdm(all_member_dsets_c, desc='ensemble', leave=False), all_member_ids)]\n",
    "    ds = merge_recursive(tqdm(var_dsets, desc='variables', leave=False))\n",
    "    all_dsets[dset_id] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

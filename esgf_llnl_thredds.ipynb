{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datetime import datetime\n",
    "#from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore', xr.SerializationWarning)\n",
    "xr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for n in tqdm(range(5)):\n",
    "    sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import requests\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# API AT: https://github.com/ESGF/esgf.github.io/wiki/ESGF_Search_REST_API#results-pagination\n",
    "\n",
    "def check_doc_for_malformed_id(d):\n",
    "    source_id = d['source_id'][0]\n",
    "    expt_id = d['experiment_id'][0]\n",
    "    if not  f\"{source_id}_{expt_id}\" in d['id']:\n",
    "        raise ValueError(f\"Dataset id {d['id']} is malformed\")\n",
    "                         \n",
    "def maybe_squeze_values(d):\n",
    "    def _maybe_squeeze(value):\n",
    "        if isinstance(value, str):\n",
    "            return value\n",
    "        try:\n",
    "            if len(value)==1:\n",
    "                return value[0]\n",
    "        except TypeError:\n",
    "            return(value)\n",
    "    return {k: _maybe_squeeze(v) for k, v in d.items()}\n",
    "                         \n",
    "def get_request(client, server, verbose=False, **payload):\n",
    "    url_keys = [] \n",
    "    url_keys = [\"{}={}\".format(k, payload[k]) for k in payload]\n",
    "    url = \"{}/?{}\".format(server, \"&\".join(url_keys))\n",
    "    if verbose:\n",
    "        print(url)\n",
    "    r = client.get(url)\n",
    "    r.raise_for_status()\n",
    "    resp = r.json()[\"response\"]\n",
    "    return resp\n",
    "\n",
    "def esgf_search(server=\"https://esgf-node.llnl.gov/esg-search/search\",\n",
    "                files_type=\"OPENDAP\", local_node=True, project=\"CMIP6\",\n",
    "                # this option should not be necessary with local_node=True\n",
    "                filter_server_url=None,\n",
    "                verbose=False, format=\"application%2Fsolr%2Bjson\",\n",
    "                use_csrf=False, **search):\n",
    "    client = requests.session()\n",
    "                         \n",
    "    payload = search\n",
    "    payload[\"project\"] = project\n",
    "    payload[\"type\"]= \"File\"\n",
    "    if local_node:\n",
    "        payload[\"distrib\"] = \"false\"\n",
    "    if use_csrf:\n",
    "        client.get(server)\n",
    "        if 'csrftoken' in client.cookies:\n",
    "            # Django 1.6 and up\n",
    "            csrftoken = client.cookies['csrftoken']\n",
    "        else:\n",
    "            # older versions\n",
    "            csrftoken = client.cookies['csrf']\n",
    "        payload[\"csrfmiddlewaretoken\"] = csrftoken\n",
    "\n",
    "    payload[\"format\"] = format\n",
    "\n",
    "    init_resp = get_request(client, server, offset=0, verbose=verbose, **payload)\n",
    "    num_found = int(init_resp[\"numFound\"])\n",
    "                         \n",
    "    offset = 0\n",
    "    all_files = []\n",
    "    files_type = files_type.upper()\n",
    "         \n",
    "    with tqdm(total=num_found, desc='ESGF Search', unit='docs') as pbar:\n",
    "        while offset < num_found:\n",
    "            resp = get_request(client, server, offset=offset, verbose=verbose, **payload)\n",
    "\n",
    "            docs = resp[\"docs\"]\n",
    "            offset += len(docs)\n",
    "            pbar.update(len(docs))\n",
    "\n",
    "            for d in docs:\n",
    "                try:\n",
    "                    check_doc_for_malformed_id(d)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                dataset_id = d['dataset_id']\n",
    "                item = OrderedDict(dataset_id=dataset_id, id=d['id'])\n",
    "                #item.update({field: d[field][0] for field in required_fields})\n",
    "                target_urls = d.pop('url')\n",
    "                item.update(maybe_squeze_values(d))\n",
    "                for f in target_urls:\n",
    "                    sp = f.split(\"|\")\n",
    "                    if sp[-1] == files_type:\n",
    "                        opendap_url = sp[0].replace('.html', '')\n",
    "                        if filter_server_url is None or filter_server_url in opendap_url:\n",
    "                            item.update({f'{files_type}_url': opendap_url})\n",
    "                            all_files.append(item)\n",
    "        pbar.close()\n",
    "    # dropping duplicates on checksum removes all identical files\n",
    "    return pd.DataFrame(all_files).drop_duplicates(subset='checksum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 233 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', variable=\"ta\", table_id='Amon', filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "## 1179 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', variable=\"ta\")\n",
    "\n",
    "## 2615 files\n",
    "files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', institution_id='IPSL',\n",
    "                    table_id='Amon', latest='true', filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "## 13453 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP',\n",
    "#                    table_id='Amon', latest='true', filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "## 83273 files\n",
    "#files = esgf_search(mip_era='CMIP6', activity_drs='CMIP', latest='true',\n",
    "#                    filter_server_url='aims3.llnl.gov')\n",
    "\n",
    "files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.experiment_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bnds_as_coords(ds):\n",
    "    new_coords_vars = [var for var in ds.data_vars if 'bnds' in var or 'bounds' in var]\n",
    "    ds = ds.set_coords(new_coords_vars)\n",
    "    return ds\n",
    "\n",
    "def fix_climatology_time(ds):\n",
    "    for dim in ds.dims:\n",
    "        if 'climatology' in ds[dim].attrs:\n",
    "            ds = ds.rename({dim: dim + '_climatology'})\n",
    "    return ds\n",
    "\n",
    "def set_coords(ds):\n",
    "    # there should only be one variable per file\n",
    "    # everything else is coords\n",
    "    varname = ds.attrs['variable_id']\n",
    "    coord_vars = set(ds.data_vars) - {varname}\n",
    "    ds = ds.set_coords(coord_vars)\n",
    "    ds = fix_climatology_time(ds)\n",
    "    return(ds)\n",
    "\n",
    "def concat_timesteps(urls):\n",
    "    urls = list(urls)\n",
    "    if len(urls) > 1:\n",
    "        ds = xr.open_mfdataset(urls, concat_dim='time',\n",
    "                                 chunks={'time': 'auto'},\n",
    "                                 preprocess=set_coords)\n",
    "        # start history fresh\n",
    "        ds.attrs['history'] = f\"{datetime.now()} xarray.open_mfdataset({urls}, concat_dim='time')\"\n",
    "    else:\n",
    "        ds = xr.open_dataset(urls[0], chunks={'time': 'auto'})\n",
    "        ds.attrs['history'] = f\"{datetime.now()} xarray.open_dataset('{urls[0]}')\"\n",
    "        ds = set_coords(ds)\n",
    "    return ds\n",
    "\n",
    "def dict_union(*dicts, merge_keys=['history', 'further_info_url'],\n",
    "               drop_keys=['DODS_EXTRA.Unlimited_Dimension']):\n",
    "    if len(dicts) > 2:\n",
    "        return reduce(dict_union, dicts)\n",
    "    elif len(dicts)==2:\n",
    "        d1, d2 = dicts\n",
    "        d = type(d1)()\n",
    "        # union\n",
    "        all_keys = set(d1) | set(d2)\n",
    "        for k in all_keys:\n",
    "            v1 = d1.get(k)\n",
    "            v2 = d2.get(k)\n",
    "            if (v1 is None and v2 is None) or k in drop_keys:\n",
    "                pass\n",
    "            elif v1 is None:\n",
    "                d[k] = v2\n",
    "            elif v2 is None:\n",
    "                d[k] = v1\n",
    "            elif v1==v2:\n",
    "                d[k] = v1\n",
    "            elif k in merge_keys:\n",
    "                d[k] = '\\n'.join([v1, v2])\n",
    "        return d\n",
    "    elif len(dicts)==1:\n",
    "        return dicts[0]\n",
    "\n",
    "def concat_ensembles(member_dsets, member_ids, join='outer'):\n",
    "    if len(member_dsets)==1:\n",
    "        return member_dsets[0]\n",
    "    concat_dim = xr.DataArray(member_ids, dims='member_id', name='member_id')\n",
    "    \n",
    "    # merge attributes\n",
    "    attrs = dict_union(*[ds.attrs for ds in member_dsets])\n",
    "    \n",
    "    # align first to deal with the fact that some ensemble members have different lengths\n",
    "    # inner join keeps only overlapping segments of each ensemble\n",
    "    # outer join gives us the longest possible record\n",
    "    member_dsets_aligned = xr.align(*member_dsets, join=join)\n",
    "    \n",
    "    # keep only coordinates from first ensemble member to simplify merge\n",
    "    first = member_dsets_aligned[0]\n",
    "    rest = [mds.reset_coords(drop=True) for mds in member_dsets_aligned[1:]]\n",
    "    objs_to_concat = [first] + rest\n",
    "    \n",
    "    ds = xr.concat(objs_to_concat, dim=concat_dim, coords='minimal')\n",
    "    attrs['history'] += f\"\\n{datetime.now()} xarray.concat(<ALL_MEMBERS>, dim='member_id', coords='minimal')\"\n",
    "    ds.attrs = attrs\n",
    "    return ds\n",
    "\n",
    "def merge_vars(ds1, ds2):\n",
    "    # merge two datasets at a time - designed for recursive merging\n",
    "    # drop all variables from second that already exist in first's coordinates\n",
    "\n",
    "    # I can't believe xarray doesn't have a merge that keeps attrs\n",
    "    attrs = dict_union(ds1.attrs, ds2.attrs)\n",
    "    \n",
    "    # non dimension coords\n",
    "    ds1_ndcoords = set(ds1.coords) - set(ds1.dims)\n",
    "    \n",
    "    # edge case for variable 'ps', which is a coordinate in some datasets\n",
    "    # and a data_var in its own dataset\n",
    "    ds2_dropvars = set(ds2.variables).intersection(ds1_ndcoords)\n",
    "    ds2_drop = ds2.drop(ds2_dropvars)\n",
    "    \n",
    "    ds = xr.merge([ds1, ds2_drop])\n",
    "    ds.attrs = attrs\n",
    "    return ds\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "def merge_recursive(dsets):\n",
    "    dsm = reduce(merge_vars, dsets)\n",
    "    dsm.attrs['history'] += f\"\\n{datetime.now()} xarray.merge(<ALL_VARIABLES>)\"\n",
    "    # fix further_info_url\n",
    "    fi_urls = set(dsm.attrs['further_info_url'].split('\\n'))\n",
    "    dsm.attrs['further_info_url'] = '\\n'.join(fi_urls)\n",
    "    return dsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fields = ['institution_id', 'source_id', 'experiment_id', 'table_id', 'grid_label']\n",
    "all_dsets = {}\n",
    "for dset_keys, dset_files in tqdm(files.groupby(dataset_fields)):\n",
    "    dset_id = '.'.join(dset_keys)\n",
    "    print(dset_id)\n",
    "    all_vars = []\n",
    "    for var_id, var_files in dset_files.groupby('variable_id'):\n",
    "        print('-', var_id)\n",
    "        member_dsets = []\n",
    "        member_ids = []\n",
    "        for m_id, m_files in var_files.groupby('member_id'):\n",
    "            print('  -', m_id, len(m_files))\n",
    "            member_ids.append(m_id)\n",
    "            member_dsets.append(concat_timesteps(m_files.OPENDAP_url))\n",
    "        dset = concat_ensembles(member_dsets, member_ids)\n",
    "        all_vars.append(dset)\n",
    "    ds_merged = merge_recursive(all_vars)\n",
    "    ds_rechunk = ds.chunk({'member_id': 1, 'time': 'auto', })\n",
    "    all_dsets[dset_id] = ds_rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
